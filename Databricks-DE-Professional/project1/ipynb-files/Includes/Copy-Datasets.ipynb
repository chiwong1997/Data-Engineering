{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb4ba01f-30f5-478a-87f5-9b59e5155be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653957a1-459b-4b99-95eb-eaf9a5b81f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb561bf4-e7cc-44c3-b221-263504ad6d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CourseDataset:\n",
    "    def __init__(self, uri, location, checkpoint_path, data_catalog, db_name):\n",
    "        self.uri = uri\n",
    "        self.location = location\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.catalog_name = data_catalog\n",
    "        self.db_name = db_name\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        source = self.uri\n",
    "        target = self.location\n",
    "        files = dbutils.fs.ls(source)\n",
    "\n",
    "        for f in files:\n",
    "            source_path = f\"{source}/{f.name}\"\n",
    "            target_path = f\"{target}/{f.name}\"\n",
    "            if not path_exists(target_path):\n",
    "                print(f\"Copying {f.name} ...\")\n",
    "                dbutils.fs.cp(source_path, target_path, True)\n",
    "    \n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.sql(f\"USE CATALOG {self.catalog_name}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.db_name}\")\n",
    "        spark.sql(f\"USE SCHEMA {self.db_name}\")\n",
    "    \n",
    "    \n",
    "    def clean_up(self):\n",
    "        print(\"Removing Checkpoints ...\")\n",
    "        dbutils.fs.rm(self.checkpoint, True)\n",
    "        print(\"Dropping Database ...\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS {self.db_name} CASCADE\")\n",
    "        print(\"Removing Dataset ...\")\n",
    "        dbutils.fs.rm(self.location, True)\n",
    "        print(\"Done\")\n",
    "\n",
    "    \n",
    "    def __get_index(self, dir):\n",
    "        try:\n",
    "            files = dbutils.fs.ls(dir)\n",
    "            file = max(f.name for f in files if f.name.endswith('.json'))\n",
    "            index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "        except:\n",
    "            index = 0\n",
    "        return index+1\n",
    "    \n",
    "    \n",
    "    def __load_json_file(self, current_index, streaming_dir, raw_dir):\n",
    "        latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "        source = f\"{streaming_dir}/{latest_file}\"\n",
    "        target = f\"{raw_dir}/{latest_file}\"\n",
    "        prefix = streaming_dir.split(\"/\")[-1]\n",
    "        if path_exists(source):\n",
    "            print(f\"Loading {prefix}-{latest_file} file to the bookstore dataset\")\n",
    "            dbutils.fs.cp(source, target)\n",
    "    \n",
    "    \n",
    "    def __load_data(self, max, streaming_dir, raw_dir, all=False):\n",
    "        index = self.__get_index(raw_dir)\n",
    "        if index > max:\n",
    "            print(\"No more data to load\\n\")\n",
    "\n",
    "        elif all == True:\n",
    "            while index <= max:\n",
    "                self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "                index += 1\n",
    "        else:\n",
    "            self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "            index += 1\n",
    "    \n",
    "    def load_new_data(self, num_files = 1):\n",
    "        streaming_dir = f\"{self.location}/kafka-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw\"\n",
    "        for i in range(num_files):\n",
    "            self.__load_data(10, streaming_dir, raw_dir)\n",
    "        \n",
    "    \n",
    "    def load_books_updates(self):\n",
    "        streaming_dir = f\"{self.location}/books-updates-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw/books-updates\"\n",
    "        self.__load_data(5, streaming_dir, raw_dir)\n",
    "        \n",
    "    def process_bronze(self):\n",
    "        schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                            .format(\"cloudFiles\")\n",
    "                            .option(\"cloudFiles.format\", \"json\")\n",
    "                            .schema(schema)\n",
    "                            .load(f\"{self.location}/kafka-raw\")\n",
    "                            .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "                            .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "                      .writeStream\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/bronze\")\n",
    "                          .option(\"mergeSchema\", True)\n",
    "                          .partitionBy(\"topic\", \"year_month\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .table(\"bronze\"))\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        \n",
    "    def __upsert_data(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "        sql_query = \"\"\"\n",
    "          MERGE INTO orders_silver a\n",
    "          USING orders_microbatch b\n",
    "          ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "          WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "        \n",
    "    def __batch_upsert(self, microBatchDF, batchId):\n",
    "        window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "        \n",
    "        (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "                     .withColumn(\"rank\", F.rank().over(window))\n",
    "                     .filter(\"rank == 1\")\n",
    "                     .drop(\"rank\")\n",
    "                     .createOrReplaceTempView(\"ranked_updates\"))\n",
    "\n",
    "        query = \"\"\"\n",
    "            MERGE INTO customers_silver c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "                WHEN MATCHED AND c.row_time < r.row_time\n",
    "                  THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED\n",
    "                  THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(query)\n",
    "        \n",
    "    \n",
    "    def __type2_upsert(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            MERGE INTO books_silver\n",
    "            USING (\n",
    "                SELECT updates.book_id as merge_key, updates.*\n",
    "                FROM updates\n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT NULL as merge_key, updates.*\n",
    "                FROM updates\n",
    "                JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "                WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "              ) staged_updates\n",
    "            ON books_silver.book_id = merge_key \n",
    "            WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "              UPDATE SET current = false, end_date = staged_updates.updated\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "              VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "    \n",
    "    def process_orders_silver(self):\n",
    "        json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "        \n",
    "        deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'orders'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"order_timestamp\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "        \n",
    "        query = (deduped_df.writeStream\n",
    "                   .foreachBatch(self.__upsert_data)\n",
    "                   .outputMode(\"update\")\n",
    "                   .option(\"checkpointLocation\", f\"{self.checkpoint}/orders_silver\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "        \n",
    "    def process_customers_silver(self):\n",
    "        \n",
    "        schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "        \n",
    "        df_country_lookup = spark.read.json(f\"{dataset_bookstore}/country_lookup\")\n",
    "\n",
    "        query = (spark.readStream\n",
    "                          .table(\"bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                          .select(\"v.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "                       .writeStream\n",
    "                          .foreachBatch(self.__batch_upsert)\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/customers_silver\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    def process_books_silver(self):\n",
    "        schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                        .table(\"bronze\")\n",
    "                        .filter(\"topic = 'books'\")\n",
    "                        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                        .select(\"v.*\")\n",
    "                     .writeStream\n",
    "                        .foreachBatch(self.__type2_upsert)\n",
    "                        .option(\"checkpointLocation\", f\"{self.checkpoint}/books_silver\")\n",
    "                        .trigger(availableNow=True)\n",
    "                        .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "    def process_current_books(self):\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE current_books\n",
    "            AS SELECT book_id, title, author, price\n",
    "               FROM books_silver\n",
    "               WHERE current IS TRUE\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed9a2d9-c3fd-425e-943a-d94577dcd40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_source_uri = \"s3://dalhussein-courses/DE-Pro/datasets/bookstore/v1/\"\n",
    "dataset_bookstore = 'dbfs:/mnt/demo-datasets/DE-Pro/bookstore'\n",
    "spark.conf.set(f\"dataset.bookstore\", dataset_bookstore)\n",
    "checkpoint_path = \"dbfs:/mnt/demo_pro/checkpoints\"\n",
    "data_catalog = 'hive_metastore'\n",
    "db_name = \"bookstore_eng_pro\"\n",
    "\n",
    "bookstore = CourseDataset(data_source_uri, dataset_bookstore, checkpoint_path, data_catalog, db_name)\n",
    "bookstore.download_dataset()\n",
    "bookstore.create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46737c07-c00d-40fb-81d4-fe6796c80e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#bookstore.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67641ce4-6c1b-4243-b2af-600416c67d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Copy-Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}